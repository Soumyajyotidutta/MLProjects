{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6nUFTsmqiDs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "EH7sbnSMtK68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker = pipeline(\"fill-mask\", model = \"bert-base-uncased\")\n",
        "input_string1 = \"This man works as a [MASK].\"\n",
        "input_string2 = \"This woman works as [MASK].\"\n",
        "result1= unmasker(input_string1)\n",
        "result2 = unmasker(input_string2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMbnjHW8tZ-K",
        "outputId": "7667ee52-6e01-4f80-fd2e-83097433bfdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([r[\"token_str\"] for r in result1])\n",
        "print([r[\"token_str\"] for r in result2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdnq40ytwctS",
        "outputId": "2a6467f2-0750-459a-c470-332f6f0479a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
            "['well', 'maid', 'nurse', 'engineer', 'translator']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IA8Ht8pdN2l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "result = unmasker(\"This man works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"This woman works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5NVn5Lfzigm",
        "outputId": "818f7efc-3d25-4f2d-cea5-0bac23f5d8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
            "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "    \"This analysis has been generated using huggingface libraries\",\n",
        "    \"TensorFlow is a good ML library\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "SFYh5a77L2rh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f1f57f-548d-4282-fb76-e8944e1ff2bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9873588681221008},\n",
              " {'label': 'POSITIVE', 'score': 0.9994980096817017}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "sZarboX_Nrv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_inputs = [\n",
        "    \"This is a test string input\",\n",
        "    \"Huggingface NLP chapter 2 rocks\",\n",
        "]\n",
        "inputs = tokenizer(my_inputs, truncation = True, padding = True, return_tensors = 'pt')"
      ],
      "metadata": {
        "id": "_dwV21qTPte4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "id": "SnsybrekTiM1",
        "outputId": "250398c6-3348-4bc2-f2eb-a3d1bd0b7044",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  2003,  1037,  3231,  5164,  7953,   102,     0],\n",
            "        [  101, 17662, 12172, 17953,  2361,  3127,  1016,  5749,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    }
  ]
}